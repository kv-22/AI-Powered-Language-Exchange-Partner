{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_LIRkEfHYsGw",
    "outputId": "5f73e624-6d9d-4a3f-c29e-a171f1863ce3"
   },
   "outputs": [],
   "source": [
    "!pip install datasets transformers sacrebleu torch sentencepiece transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7GmHd6aaZygy",
    "outputId": "e2c67840-0018-4214-aee6-149abfb50a08"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')  # Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "KLBhgR5YbRZe",
    "outputId": "3ec4f6c6-0938-46fb-f465-47b9a550aa0d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df= pd.read_excel(\"/content/drive/My Drive/combined.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "id": "irn9g48kbjUo",
    "outputId": "5780e3cb-425e-438e-c266-12b517d1211a"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "danoSCjKaZAz",
    "outputId": "ac07a2aa-4c3f-4a8b-dc17-ac3fd73f47e4"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "my_dataset = Dataset.from_pandas(df)\n",
    "print(my_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f2zWpk9Rh3wS",
    "outputId": "e5385537-b1d2-44b8-dac6-1b81476f6e67"
   },
   "outputs": [],
   "source": [
    "!pip install PyArabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "a78d0f3f87ba445aaf8ae261c267905b",
      "b5416fcd5b774fab89ef1e6fd571b28c",
      "a47c2d40fc7b4b33ae236c61966db18e",
      "504a83b40efd4106936e6bc3184937f8",
      "1fd5150ce8b740a5a57a12e3bb509ca4",
      "d50d1fb5092e4300b06bf8d99e605553",
      "d7c6c098b6b0410495a6875b4a3c85a3",
      "dd3b1a1bb9784790956099453104e1cf",
      "af82002d789c4ec69682226007353eaa",
      "f1f44311be5e47bf92d6ceb6601854d4",
      "3268f982e6454b06aa71bc49b2f14336"
     ]
    },
    "id": "un1FDvWxftqN",
    "outputId": "6562d972-8c2c-457d-d8a0-d12205e011b9"
   },
   "outputs": [],
   "source": [
    "import pyarabic.araby as araby\n",
    "import re\n",
    "\n",
    "def clean_arabic(example):\n",
    "  # example['Text (Arabic)'] = re.sub(r'[.?،,!:\\\"\\'؟]', '', example['Text (Arabic)'])\n",
    "  example['Text (Arabic)'] = araby.strip_diacritics(example['Text (Arabic)'])\n",
    "  return example\n",
    "\n",
    "my_dataset = my_dataset.map(clean_arabic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ij2__EJ4ehhs",
    "outputId": "c06cec6c-aa31-48b3-e736-c9679d0ae962"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# filter rows that have anything other than letters\n",
    "def filter_dataset(dataset):\n",
    "    filtered_rows = []\n",
    "    for i in range(len(dataset['Text (Arabic)'])):\n",
    "        if bool(re.search(r'[^\\u0621-\\u064A\\s]', dataset['Text (Arabic)'][i])):\n",
    "            filtered_rows.append(i)\n",
    "\n",
    "    return dataset.select(filtered_rows)\n",
    "\n",
    "filtered_dataset2 = filter_dataset(my_dataset)\n",
    "\n",
    "print(filtered_dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Ko3tixRfX4O"
   },
   "outputs": [],
   "source": [
    "for i in range(len(filtered_dataset2)):\n",
    "  print(filtered_dataset2[i]['Text (Arabic)'])\n",
    "  print(filtered_dataset2[i]['Text (English)'])\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L7XhJBgIxqMo",
    "outputId": "3b934bdf-e4ee-4861-f658-58dbc64e4a02"
   },
   "outputs": [],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "cc3b3317df634585aabef75933d2a0a5",
      "e345272ac85b4294a4b577b168bb01a4",
      "a78422dd2f4e40e5ac33d7e7994e6fc7",
      "9af1af7aa02d4d968e5bceda6aa774b9",
      "e01a54277aff4540a592e80a87f9ab18",
      "c1bfd47895c7409f8a463816bfcfa4a8",
      "97db09a29360418bad4c75018960785b",
      "76e499f08c374c2e86c25e707ffe1fff",
      "3c32562c18114321ba30fd26e2390f3f",
      "cf4626c3ee164c09aa9cbd0021c8a2d3",
      "1cbf8f97894f49258eb20e5d6ebc6b02"
     ]
    },
    "id": "qzB5_GbHnZjB",
    "outputId": "a1f317e3-7cda-4e91-d0c0-4de85d4c39b0"
   },
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "def clean_english(example):\n",
    "  example['Text (English)'] = contractions.fix(example['Text (English)']) # this expands words like I'm to I am\n",
    "  # example['Text (English)'] = re.sub(r'[:.?,\\'!;()‘’“”\"*…]', '', example['Text (English)'])\n",
    "  # example['Text (English)'] = re.sub(r'[-—]', ' ', example['Text (English)'])\n",
    "  example['Text (English)'] = re.sub(r'[()*…]', '', example['Text (English)'])\n",
    "  example['Text (English)'] = re.sub(r'[%]', ' percent', example['Text (English)'])\n",
    "  example['Text (English)'] = example['Text (English)'].strip()\n",
    "  return example\n",
    "\n",
    "my_dataset = my_dataset.map(clean_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MQuxO7Z7n_uI",
    "outputId": "865b21f3-4931-4d60-8aac-80d6fe6f4259"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# filter rows that have anything other than letters\n",
    "def filter_dataset(dataset):\n",
    "    filtered_rows = []\n",
    "    for i in range(len(dataset['Text (English)'])):\n",
    "        if bool(re.search(r'[^A-Za-z\\s]', dataset['Text (English)'][i])):\n",
    "            filtered_rows.append(i)\n",
    "\n",
    "    return dataset.select(filtered_rows)\n",
    "\n",
    "filtered_dataset2 = filter_dataset(my_dataset)\n",
    "\n",
    "print(filtered_dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-FrPDxznu3X"
   },
   "outputs": [],
   "source": [
    "for i in range(len(filtered_dataset2)):\n",
    "  print(filtered_dataset2[i]['Text (Arabic)'])\n",
    "  print(filtered_dataset2[i]['Text (English)'])\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lI3NPuTRdJaw",
    "outputId": "452679b5-f643-47b5-ca55-1b021397902f"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint=\"Helsinki-NLP/opus-mt-ar-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XXuyx3i62_hw",
    "outputId": "242f4695-f6ec-4153-b267-42d450f59a8f"
   },
   "outputs": [],
   "source": [
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0n8zfM9skyve",
    "outputId": "58f5ccb3-d6ad-4e29-eaa0-d608e033bc0c"
   },
   "outputs": [],
   "source": [
    "print(my_dataset[0]['Text (Arabic)']+\"\\n\")\n",
    "print(my_dataset[0]['Text (English)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xpPZZYWIjVco",
    "outputId": "c102f7ee-ce35-472f-e5fd-1b3645afb749"
   },
   "outputs": [],
   "source": [
    "tokenizer.tokenize(my_dataset[0]['Text (Arabic)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aI-N0eJmlerl",
    "outputId": "69ce1a2d-3772-48d7-f142-9ba930036009"
   },
   "outputs": [],
   "source": [
    "tokenizer.tokenize(my_dataset[0]['Text (English)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VJES62FOoSUj",
    "outputId": "20509e90-1a45-4eb4-8bb7-87c12146dde3"
   },
   "outputs": [],
   "source": [
    "len(tokenizer.tokenize(my_dataset['train'][1]['Text (Arabic)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2XwxmJRsoyMY",
    "outputId": "570d3ea5-a953-425e-db66-bf41d9005964"
   },
   "outputs": [],
   "source": [
    "# Tokenize the sentences and calculate their lengths to find max length\n",
    "tokenized_lengths = [len(tokenizer.encode(sentence)) for sentence in my_dataset['train']['Text (Arabic)']] # for eng do 'Text (English)'\n",
    "\n",
    "print(\"Tokenized Lengths of Sentences:\", tokenized_lengths)\n",
    "\n",
    "import numpy as np\n",
    "print(\"Mean Length:\", np.mean(tokenized_lengths))\n",
    "print(\"Max Length:\", np.max(tokenized_lengths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GYu47_Bga2-Y",
    "outputId": "1fb82cb8-404d-4c5c-b18f-d2c2ac4d04d6"
   },
   "outputs": [],
   "source": [
    "my_dataset = my_dataset.train_test_split(test_size=0.3)\n",
    "print(my_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zmtb0bAwMA-J",
    "outputId": "517cf963-dd8a-4d2f-ac26-198279798a6a"
   },
   "outputs": [],
   "source": [
    "my_dataset_test = my_dataset['test'].train_test_split(test_size=0.5)\n",
    "print(my_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Nr97uZJMTGS",
    "outputId": "9dffd55d-236c-44a9-f56f-09e6add69b60"
   },
   "outputs": [],
   "source": [
    "my_dataset['validation'] = my_dataset_test['train']\n",
    "my_dataset['test'] = my_dataset_test['test']\n",
    "print(my_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIH7lCnk3Qua"
   },
   "outputs": [],
   "source": [
    "max_input_length = 195\n",
    "max_target_length = 360\n",
    "source_lang = \"ar\"\n",
    "target_lang = \"en\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex for ex in examples[\"Text (Arabic)\"]]\n",
    "    targets = [ex for ex in examples[\"Text (English)\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169,
     "referenced_widgets": [
      "62040dcc0ae24d87b42a138df20add8a",
      "8bc3695358fd412aa82bbe9c15e34d55",
      "86b3bd9257804bdd88041c28f59a8b6b",
      "9e0764c7cb164649857ba916356ec297",
      "1e411e73ee534a9192128fb1f024c34b",
      "b0c0a45eef1c470494006408297a0e78",
      "184023b459dc4d34bd6f5872fb96f62b",
      "ae464fbe993e47179db6cd945c542958",
      "9591f90f8cff42cb9af4353efbae2839",
      "2a48d87e90ca4f35b1599f05bf90a0cd",
      "965d237ecfc1476abb541092c6d66eaf",
      "262b425fb6ed4325a1a261f0025dc99c",
      "4243ccaa54604c569986419309a9f53f",
      "4305fb3496764f6699e95d33a51e2308",
      "09ac2530bee64a3bbbdf6f4ce9d908d6",
      "0dedef4e4abf49c0bdaad04e4ed65f53",
      "f300b13a190a4d78b300acc1b952f1dd",
      "23062fcba5e642deb368110a6a120ce7",
      "a5a6a8e0f04046ffb61c889f1279b3e7",
      "ceb1c088dcfe486bb6e9f42e3767a809",
      "833c450ed0b54318ba3be8f74db7c02d",
      "c21bc72b14c6448ea28d69b3daaf1809",
      "1f7893856608481abe2efaf1be496d9e",
      "8b1406a65d1242fabd0017e5042a8452",
      "7343a86d0ce44cabba2d97b821bb20c5",
      "d79ad5bdf83042a1b9a192e69e872eb1",
      "40f9cf5bc12743fda350a83c984ff071",
      "4dd94b8fc75046e1a86ec3f4c9e53cde",
      "9e5b2362267b495c8e191536ce15b877",
      "5bdcf7e4b5b34eacaf07f3f7840d29e8",
      "5b399ffcb54a48098b59b7b1699fca5f",
      "5911d59b65634e90a9d51691e2dabe50",
      "5be3c8bafe104036926b599d88ab33c5"
     ]
    },
    "id": "KOT9f3rm4Gmg",
    "outputId": "9815342d-433f-42b7-9332-5b6514b2fa54"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = my_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOGiKe8L4Mca"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KP1H6F7o43v2"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sT79BlGc7tiF",
    "outputId": "ae108137-29d5-462e-d0dd-93af334fd97c"
   },
   "outputs": [],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RKcfGpB-7tBd",
    "outputId": "168e16c8-dc66-43aa-d5cf-b558dcbeecc5"
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "bleu = load(\"bleu\")\n",
    "meteor = load('meteor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WyzEPubiqNq9",
    "outputId": "5da22d43-b26f-4b9b-fe85-f7eb83bda95f"
   },
   "outputs": [],
   "source": [
    "fake_preds = [\"Is there elevator?\", \"I've seen him before.\"]\n",
    "fake_labels = [[\"Is there an elevator?\"], [\"I've seen him before.\"]]\n",
    "\n",
    "print(bleu.compute(predictions=fake_preds, references=fake_labels))\n",
    "print(meteor.compute(predictions=fake_preds, references=fake_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7f-lC4v5BPd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "  preds = [pred.strip() for pred in preds]\n",
    "  labels = [[label.strip()] for label in labels]\n",
    "  return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "  preds, labels = eval_preds\n",
    "  if isinstance(preds, tuple):\n",
    "      preds = preds[0]\n",
    "  decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "  # Replace -100 in the labels as we can't decode them.\n",
    "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "  # Some simple post-processing\n",
    "  decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "  result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "  result_meteor = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "  result = {\"bleu\": result[\"bleu\"]}\n",
    "  result['meteor'] = result_meteor['meteor']\n",
    "  prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "  result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "  result = {k: round(v, 4) for k, v in result.items()}\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "4a3481272c224a779cf593d086cec504",
      "1413e50567694928be57101405013bfc",
      "e2c3e1ea5f4d41f593f54f79242aeb7f",
      "c4195af66d4d48e9b664754b80ea422d",
      "a076d9fdc87f4d69a5ea6ed1b5ce1208",
      "56535072fe7f42fd9b92250eff7ed728",
      "e9b2c1fffa37437f9495f1f6d59719af",
      "4bbc2142f2824ae3a4f9c3f7b5bea4eb",
      "45e13597389248e1bc2c9488c64e7957",
      "0df85f132f304e6e97d7094f8efee28d",
      "9c0b13e6b892442889356497dcb7c35e",
      "66f55b72e62c4cdc843fa5c271eb21af",
      "2de7e3899c1048dfa257ddbb9d9782e5",
      "31a66c5af80f43238e5825c5995ffa3e",
      "e73a3fdc91204447bbc850ca230400c9",
      "2c863d89cc5740b88467a497c123c645",
      "5d5888ca756b49fe8e7c841f9d06e9ed",
      "bd53a3c547ae4d54b3867829ef25c9d1",
      "24183f7f009147feacfccf717aa437f6",
      "b94b8160a71c4a9187f18005379c410a"
     ]
    },
    "id": "OKBrb28e8JbR",
    "outputId": "cf6e3c2c-0e43-45d1-e813-23adc3700ff9"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u6P61Mj-4U75",
    "outputId": "a74b1ee6-b753-4de4-9411-eeb7229ad3db"
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# try diff parameters\n",
    "batch_size = 4\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}-5\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.001,\n",
    "    num_train_epochs=10,\n",
    "    predict_with_generate=True,\n",
    "    push_to_hub=True,\n",
    "    warmup_steps=20,\n",
    "    lr_scheduler_type=\"linear\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CdBjdpom5DX1",
    "outputId": "0f4628fd-a93b-4e96-885e-2c509ceea536"
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZIv0LbKc7hzQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your API key\n",
    "os.environ[\"WANDB_API_KEY\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "id": "2vDVE1Bd6zLA",
    "outputId": "76c20025-302e-481f-8aaa-c47b01ca94a2"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import time\n",
    "\n",
    "# run this before every run so it doesn't overwrite the previous run\n",
    "wandb.init(project=\"huggingface\", name=f\"run_{int(time.time())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "YA4x8ve_5Gm_",
    "outputId": "72708d30-f749-48fb-cbd4-d8fdedab87d2"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120,
     "referenced_widgets": [
      "28f0da6ce9b8492d87b7ae5f40f1ecd0",
      "7da04fe42b294ab89207c8e8571e4e78",
      "be3340a9942a4daaa2cd66399ed8bb7e",
      "8171be7d1873408984842a7c711d77eb",
      "18981c18565e4c7ea5742fb9317f2aeb",
      "5d7995af011347529fcfae5047fc9414",
      "f2f76fe1281647c6a998d528b1aaa2ee",
      "f6c6e0f8281b4b09a80e1d3570c7c943",
      "a09c961bace743879680c5b0a982f9d0",
      "5b36b8def0294af788ec62bd9ccac32a",
      "a1c9716dde804c12b5cb5ef87d3623f8"
     ]
    },
    "id": "eqAfW2Evwgym",
    "outputId": "f440f3e0-7f86-46b5-b4fa-f00264bf58c7"
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ARrUmTlwwDbL",
    "outputId": "ead36f1e-141b-4cbf-80f5-9828ade68a99"
   },
   "outputs": [],
   "source": [
    "my_dataset['test'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zt1tysnY8p6m",
    "outputId": "1103bbe9-13fc-4d54-dcf6-affd7b870c8f"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# check some translations with trained model\n",
    "src_text = my_dataset['test'][2]['Text (Arabic)']\n",
    "\n",
    "model_checkpoint = \"the one u saved on hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "\n",
    "translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True)) # can try more parameters\n",
    "[tokenizer.decode(t, skip_special_tokens=True) for t in translated]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6usbHFqEESdV"
   },
   "source": [
    "Referneces:\n",
    "\n",
    "-https://medium.com/@tskumar1320/how-to-fine-tune-pre-trained-language-translation-model-3e8a6aace9f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWsoq-D3xDNX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
