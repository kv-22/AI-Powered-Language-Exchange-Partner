{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_LIRkEfHYsGw",
    "outputId": "f16aef1a-2b86-4243-98a3-956076028de6"
   },
   "outputs": [],
   "source": [
    "!pip install datasets transformers sacrebleu torch sentencepiece transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7GmHd6aaZygy",
    "outputId": "208b3ba1-ae4a-4c7a-cf47-4179eb5a0f61"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')  # Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "KLBhgR5YbRZe",
    "outputId": "f0a0c5eb-ba46-43ec-a5a4-3aced1212774"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df= pd.read_excel(\"/content/drive/My Drive/combined.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "id": "irn9g48kbjUo",
    "outputId": "0b7fb15f-abda-47ca-de53-3c9e9e55b5e0"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "danoSCjKaZAz",
    "outputId": "eacb0875-fc7f-488e-f33e-46e04a3d948e"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "my_dataset = Dataset.from_pandas(df)\n",
    "print(my_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f2zWpk9Rh3wS",
    "outputId": "a311fff0-a7af-45f3-8654-61f9faa60415"
   },
   "outputs": [],
   "source": [
    "!pip install PyArabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "2ab6441e77424a0ab73e2b076271e8f1",
      "42925b5cb15d4dab8745c19d7533248f",
      "8812314b1ed043d5963a96afd539c513",
      "248f23ebff054238a2930a4f6694e5ee",
      "e191deebce6846ada3fe928e2f4d149b",
      "89d4866943d44ec3a32206807d4bdc42",
      "e46c091754254ce89bb99d3901ef59d1",
      "55dc33bf41a34869a3557340e3dc3ace",
      "312612fa545a4b68b380ff90e5442872",
      "f00c271b65914784ba7f46fdfa011b27",
      "a08e6aa16fab40e298e9be7b6b75905b"
     ]
    },
    "id": "un1FDvWxftqN",
    "outputId": "2b25c372-6340-4435-8c83-d91663708fa7"
   },
   "outputs": [],
   "source": [
    "import pyarabic.araby as araby\n",
    "import re\n",
    "\n",
    "def clean_arabic(example):\n",
    "  # example['Text (Arabic)'] = re.sub(r'[.?،,!:\\\"\\'؟]', '', example['Text (Arabic)'])\n",
    "  example['Text (Arabic)'] = araby.strip_diacritics(example['Text (Arabic)'])\n",
    "  return example\n",
    "\n",
    "my_dataset = my_dataset.map(clean_arabic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ij2__EJ4ehhs",
    "outputId": "c06cec6c-aa31-48b3-e736-c9679d0ae962"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# filter rows that have anything other than letters\n",
    "def filter_dataset(dataset):\n",
    "    filtered_rows = []\n",
    "    for i in range(len(dataset['Text (Arabic)'])):\n",
    "        if bool(re.search(r'[^\\u0621-\\u064A\\s]', dataset['Text (Arabic)'][i])):\n",
    "            filtered_rows.append(i)\n",
    "\n",
    "    return dataset.select(filtered_rows)\n",
    "\n",
    "filtered_dataset2 = filter_dataset(my_dataset)\n",
    "\n",
    "print(filtered_dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Ko3tixRfX4O"
   },
   "outputs": [],
   "source": [
    "for i in range(len(filtered_dataset2)):\n",
    "  print(filtered_dataset2[i]['Text (Arabic)'])\n",
    "  print(filtered_dataset2[i]['Text (English)'])\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L7XhJBgIxqMo",
    "outputId": "7f70fc75-eb6b-4d2d-cb2a-ca1680644ee9"
   },
   "outputs": [],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "e8068ea7d45c4ef0bac83663369391ee",
      "62499b44ffe74d99b8be5b3219c8aeee",
      "a7b7bc515ca44a1b93c32e2b85fd1407",
      "f4dacdc5d1a549b88c24f8872c9f7aa6",
      "666d1264b0654ece8f0b09e0c36fb559",
      "48180a703f7a447199c58e5c63a75995",
      "7ba9bcc0de5142159724d4e2033df70d",
      "27dc61bf5e08453aa4df11e2a395e981",
      "66333e9b6ecb4c39b6db6d7210eb02f8",
      "1b1ad8379f4b489caa8c0e7c23f54979",
      "33c7ed58250f439dbb1428b50b0ddc86"
     ]
    },
    "id": "qzB5_GbHnZjB",
    "outputId": "4eb65bcf-2466-4713-edcc-a88995c3f9e8"
   },
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "def clean_english(example):\n",
    "  example['Text (English)'] = contractions.fix(example['Text (English)']) # this expands words like I'm to I am\n",
    "  # example['Text (English)'] = re.sub(r'[:.?,\\'!;()‘’“”\"*…]', '', example['Text (English)'])\n",
    "  # example['Text (English)'] = re.sub(r'[-—]', ' ', example['Text (English)'])\n",
    "  example['Text (English)'] = re.sub(r'[()*…]', '', example['Text (English)'])\n",
    "  example['Text (English)'] = re.sub(r'[%]', ' percent', example['Text (English)'])\n",
    "  example['Text (English)'] = example['Text (English)'].strip()\n",
    "  return example\n",
    "\n",
    "my_dataset = my_dataset.map(clean_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MQuxO7Z7n_uI",
    "outputId": "865b21f3-4931-4d60-8aac-80d6fe6f4259"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# filter rows that have anything other than letters\n",
    "def filter_dataset(dataset):\n",
    "    filtered_rows = []\n",
    "    for i in range(len(dataset['Text (English)'])):\n",
    "        if bool(re.search(r'[^A-Za-z\\s]', dataset['Text (English)'][i])):\n",
    "            filtered_rows.append(i)\n",
    "\n",
    "    return dataset.select(filtered_rows)\n",
    "\n",
    "filtered_dataset2 = filter_dataset(my_dataset)\n",
    "\n",
    "print(filtered_dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-FrPDxznu3X"
   },
   "outputs": [],
   "source": [
    "for i in range(len(filtered_dataset2)):\n",
    "  print(filtered_dataset2[i]['Text (Arabic)'])\n",
    "  print(filtered_dataset2[i]['Text (English)'])\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340,
     "referenced_widgets": [
      "647f78156b1b49238c00d0a3c16dce82",
      "67000793fedf4fdcad2e83a02b775e0d",
      "0d66c449071b416ab501e75a505a9762",
      "8da0df2a68c94dfd80f8edbe10dc27d6",
      "b85e1a22acfd4712920b71a5108da61a",
      "c9ae6bb92f1948f7b158ebc35849d87e",
      "c9be4b6818c74de78853a551b9a0a79c",
      "82fcf0d6099443099935ce8be404a360",
      "0e82a05b0795474bab8be665d0137235",
      "81199fba6358478bbdf85b100fee0a98",
      "07cb169cf1cb4469854b5806245e15e3",
      "c7b782f0a12c476aa1b209142560a564",
      "09b46c5b91164d9a9e132112a310f464",
      "e9c511d86a7348ad93de7b31f9f56d53",
      "8cf136440a6c48f589fb2800ec237257",
      "0ada4a2495dd4a969e6a06f72986a164",
      "06c39a6afdf542649635a2b7b0fb2014",
      "05667ca3b579454cb99f4a645878bc7f",
      "7ef9675d54cb4893a0e88bfd462042ca",
      "fdf9230ef1514a13bc71be3290ae68cf",
      "236151cefeca4e5a91b75b4f36c18150",
      "e74d42e7c4704b4e875dac64f4bf47be",
      "833fa28194e3472f8a480823d546801f",
      "b94ecec79a8548e7850ec0d7cdb34c6c",
      "e0d6c23f21c2497e9f89061d3cc69eb9",
      "f5a80ca9bfa74a339ad3cf7f0a0a5340",
      "4d766153762a42dfb08a99e866eb6975",
      "caa867fe38864a0ea7b151d63dc5c695",
      "22fcf46bb98f4ae58f3dd47125306a2d",
      "34cd260ce64c4778933f1b0744c9cf4b",
      "a386462586104bc9842eca0ebcbf0fc6",
      "08a06f57cf944285a5e945b895f2dec7",
      "15de412fdf1e4d8893260f9cbaa970f6",
      "a8d82dc06b944f898e43f850e3138651",
      "91eb1f9ecab74859be55dc5112237b9d",
      "20278858be98479da4744cf5b14da426",
      "1f8ee654973b49f6a2869c79e0654acc",
      "931f044a32704e548c4af774a2b9569c",
      "2da3abd061d745a9a184074ef812e428",
      "e56554591c4c46ab8912b544ffd29b0c",
      "e3497c47a6d84f768f7e3fac839dcf4f",
      "35573e62ca0a4d86a4f3fbf625a8ab33",
      "0673ba1b10bf4a54a751a439c5fb13b0",
      "46b8260bd685480d9ac686f217865bfd",
      "0c4de1e44fe14ed5a694ab6431f73f7b",
      "460fdff66b46486b83774b15c472c9b0",
      "bf312ff157aa40b4823b06c3b2e22b17",
      "23002ba15f7a4289af33a5a864dc374c",
      "46856d9c8ea24c65ba03b3b2bf508465",
      "270c5939dbdc4803b3251fdb802f20d7",
      "5b7c77640f944411a06aa9f911c76b60",
      "6e2f009a40ce4e7bb6452ad499970896",
      "d1d00e56965049fda636b7c09343746c",
      "21834d17fbbc46fdb4dfe5e83e3fae9e",
      "55852f10d90e4058814ca10add666f53"
     ]
    },
    "id": "lI3NPuTRdJaw",
    "outputId": "ca6fc573-d61c-40bf-e707-86de6833a721"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint=\"Helsinki-NLP/opus-mt-ar-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XXuyx3i62_hw",
    "outputId": "804d4824-edf2-4415-d5c1-684bc1fd3a7d"
   },
   "outputs": [],
   "source": [
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0n8zfM9skyve",
    "outputId": "58f5ccb3-d6ad-4e29-eaa0-d608e033bc0c"
   },
   "outputs": [],
   "source": [
    "print(my_dataset[0]['Text (Arabic)']+\"\\n\")\n",
    "print(my_dataset[0]['Text (English)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xpPZZYWIjVco",
    "outputId": "c102f7ee-ce35-472f-e5fd-1b3645afb749"
   },
   "outputs": [],
   "source": [
    "tokenizer.tokenize(my_dataset[0]['Text (Arabic)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aI-N0eJmlerl",
    "outputId": "c1f5cc02-c322-4169-83df-e0964191e62d"
   },
   "outputs": [],
   "source": [
    "tokenizer.tokenize(my_dataset[0]['Text (English)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VJES62FOoSUj",
    "outputId": "20509e90-1a45-4eb4-8bb7-87c12146dde3"
   },
   "outputs": [],
   "source": [
    "len(tokenizer.tokenize(my_dataset['train'][1]['Text (Arabic)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2XwxmJRsoyMY",
    "outputId": "570d3ea5-a953-425e-db66-bf41d9005964"
   },
   "outputs": [],
   "source": [
    "# Tokenize the sentences and calculate their lengths to find max length\n",
    "tokenized_lengths = [len(tokenizer.encode(sentence)) for sentence in my_dataset['train']['Text (Arabic)']] # for eng do 'Text (English)'\n",
    "\n",
    "print(\"Tokenized Lengths of Sentences:\", tokenized_lengths)\n",
    "\n",
    "import numpy as np\n",
    "print(\"Mean Length:\", np.mean(tokenized_lengths))\n",
    "print(\"Max Length:\", np.max(tokenized_lengths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GYu47_Bga2-Y",
    "outputId": "643cdcbe-0650-4991-e965-4e520d6f3595"
   },
   "outputs": [],
   "source": [
    "my_dataset = my_dataset.train_test_split(test_size=0.2)\n",
    "print(my_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIH7lCnk3Qua"
   },
   "outputs": [],
   "source": [
    "max_input_length = 195\n",
    "max_target_length = 360\n",
    "source_lang = \"ar\"\n",
    "target_lang = \"en\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex for ex in examples[\"Text (Arabic)\"]]\n",
    "    targets = [ex for ex in examples[\"Text (English)\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137,
     "referenced_widgets": [
      "4f9610de2726403aa852d763b3a28cfd",
      "72dd80837e9d4f368225d32fe00f4cb9",
      "4c380b2846cd4f7c94f8f703b9cb43d0",
      "fafa812e26eb49daab8e96fa8fe087b8",
      "f1060db50291426d8cc484a4c30dcbaa",
      "5c7e2c619ea144fdad038d0a0124fb62",
      "e1ec60bc40994b9584ab496d7f27a2b9",
      "9bbf7ea99abc4ff9aae24ccb0fafcb35",
      "dc057d7c90024a349012be267f2d623f",
      "bc064afd1fba4efd93815e22742576f5",
      "3d5070640a324d6fbc75b0cff496fe9c",
      "4d5b8ef7f65b43c38fe7a8c735d2e3bc",
      "0ba11eab12064a528cf4e601bd384408",
      "71affc4d3e5f425aaf3b38ed11ff9679",
      "ea6416278f0343fd98c0f8391588bd7a",
      "b9e7444f36ca4a4b9845ea62c581d07d",
      "303e66354cb041f09a1fae8df5963657",
      "cba53a4dce21494ca420680499b77225",
      "ec050a1ee48e437ebef18defc95392d9",
      "443e44cf94e54f8f847741e6a302344f",
      "7473814d4e4645b4895024b1e336ea88",
      "83c8b617d65b466fbc4fb1f97a3713f8"
     ]
    },
    "id": "KOT9f3rm4Gmg",
    "outputId": "b7beb215-a83a-4d6c-9025-ada4128eeff8"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = my_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOGiKe8L4Mca"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KP1H6F7o43v2"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sT79BlGc7tiF",
    "outputId": "533291b3-34b8-4c78-ca08-91444aab4fa5"
   },
   "outputs": [],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKcfGpB-7tBd"
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "bleu = load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WyzEPubiqNq9",
    "outputId": "f37a0abd-a1bd-496c-d594-8b6f2299f7ab"
   },
   "outputs": [],
   "source": [
    "fake_preds = [\"Is there elevator?\", \"I've seen him before.\"]\n",
    "fake_labels = [[\"Is there an elevator?\"], [\"I've seen him before.\"]]\n",
    "\n",
    "bleu.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7f-lC4v5BPd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "  preds = [pred.strip() for pred in preds]\n",
    "  labels = [[label.strip()] for label in labels]\n",
    "  return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "  preds, labels = eval_preds\n",
    "  if isinstance(preds, tuple):\n",
    "      preds = preds[0]\n",
    "  decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "  # Replace -100 in the labels as we can't decode them. # when was this set?\n",
    "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "  # Some simple post-processing\n",
    "  decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "  result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "  result = {\"bleu\": result[\"bleu\"]}\n",
    "  prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "  result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "  result = {k: round(v, 4) for k, v in result.items()}\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "086ee05d58b34f54bb048d39a5d65a87",
      "b19176141fc34d299ab7a952fbd33cf6",
      "6dd2cd9c0f224e26b5dcd7d1e8d48c3a",
      "3150c0c97f7b423c9bdf368422eae484",
      "1f3896faa61346a1ad2b54948102fa26",
      "f191a8730e2645c7b0431812ed1aa9b3",
      "6999402e51e64bc0affde49311f1e9ec",
      "f6526d94026b4d2f8e2f82b1dda5c456",
      "512e5a449d5a424c9ee2419abae99eae",
      "2c360927de0c4a34be0b4c9c95ec5955",
      "88497cbf3b874dc887add37449fd1555",
      "69f053e1d3754826817f9a38d47f7037",
      "a299ab906e2d45c0afb96f69fcb0d0ad",
      "cfb6638718114e6784d9822020d0a512",
      "aff90185bed1433c8a77e00353d35780",
      "3e2ebe77c3004c10bb1e323d20bcd903",
      "e7f634540e204f8da1fef78b5a03531b",
      "d1119605ddd74f26be115435e0ee73eb",
      "73687a0174424350b4de53d2ba92ee34",
      "341d9668105349fca302635093001fd6"
     ]
    },
    "id": "OKBrb28e8JbR",
    "outputId": "8264abc0-0180-4ac3-eadd-87e92f7201cd"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u6P61Mj-4U75",
    "outputId": "bfebd0a2-64de-4990-ac04-86b6ce01565d"
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# try diff parameters, warm up steps etc\n",
    "batch_size = 4\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}-2\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5, # try diff values\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01, # try diff values\n",
    "    num_train_epochs=2,  # try diff values\n",
    "    predict_with_generate=True,\n",
    "    push_to_hub=True,\n",
    "    report_to=[\"tensorboard\"]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CdBjdpom5DX1",
    "outputId": "cfc5488f-058c-497e-fff4-d8a2f37beb9b"
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "id": "YA4x8ve_5Gm_",
    "outputId": "41a3145d-e653-47bd-e0a4-89b1eedfc265"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120,
     "referenced_widgets": [
      "d5d30d24bcee4e64a0c7f9db8a7eb812",
      "64c8ce8bc4534843a8f5639d75f9a31b",
      "7eae57c076cf4484a5a8a889a5dc14ec",
      "5b9a14bfc7be4a4bbfe5fa5e2f16546b",
      "29da6e2856b54a3591d4b0c26f6175b2",
      "2cb9577d844d476d8a632e2fc1989065",
      "6e225bcdc7aa4b49a977bedbd351cee2",
      "952ca81f3fc942fcaca236bbcd604dd3",
      "a258f05e77cd471d81f33fd503885333",
      "b7d69a14e87045b8bbac9c361f9c0078",
      "3ac2c78baec44b31952d9ae02f09ea28"
     ]
    },
    "id": "eqAfW2Evwgym",
    "outputId": "929d7b71-bad0-4c32-a5f6-5fc660f47567"
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ARrUmTlwwDbL",
    "outputId": "ead36f1e-141b-4cbf-80f5-9828ade68a99"
   },
   "outputs": [],
   "source": [
    "y_dataset['test'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zt1tysnY8p6m",
    "outputId": "1103bbe9-13fc-4d54-dcf6-affd7b870c8f"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# check some translations with trained model\n",
    "src_text = my_dataset['test'][2]['Text (Arabic)']\n",
    "\n",
    "model_checkpoint = \"the one u saved on hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "\n",
    "translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True)) # can try more parameters\n",
    "[tokenizer.decode(t, skip_special_tokens=True) for t in translated]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6usbHFqEESdV"
   },
   "source": [
    "Referneces:\n",
    "\n",
    "-https://medium.com/@tskumar1320/how-to-fine-tune-pre-trained-language-translation-model-3e8a6aace9f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWsoq-D3xDNX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
